---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: minio
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 3.7.1
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  values:
    controllers:
      minio:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: quay.io/minio/minio
              tag: RELEASE.2025-02-28T09-55-16Z@sha256:a929054ae025fa7997857cd0e2a2e3029238e31ad89877326dc032f4c1a14259
            env:
              TZ: ${TZ}
              MINIO_API_CORS_ALLOW_ORIGIN: https://minio.${SECRET_DOMAIN},https://s3.${SECRET_DOMAIN}
              MINIO_BROWSER_REDIRECT_URL: https://minio.${SECRET_DOMAIN}
              MINIO_PROMETHEUS_JOB_ID: minio
              MINIO_PROMETHEUS_URL: http://kube-prometheus-stack-prometheus.observability:9090
              MINIO_PROMETHEUS_AUTH_TYPE: public
              # MINIO_SERVER_URL: https://s3.${SECRET_DOMAIN}
              MINIO_UPDATE: "off"
              MINIO_API_REQUESTS_MAX: "1600"
            envFrom:
              - secretRef:
                  name: minio-secret
            args: ["server", "/data", "--console-address", ":9001"]
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /minio/health/live
                    port: 9000
                  initialDelaySeconds: 30
                  periodSeconds: 30
                  timeoutSeconds: 10
                  failureThreshold: 6
              readiness: *probes
            resources:
              requests:
                cpu: 100m
              limits:
                memory: 2Gi
    # defaultPodOptions:
    #   securityContext:
    #     runAsNonRoot: true
    #     runAsUser: 568
    #     runAsGroup: 568
    #     fsGroup: 568
    #     fsGroupChangePolicy: OnRootMismatch
    #     supplementalGroups: [10000]
    #     seccompProfile: { type: RuntimeDefault }
    service:
      app:
        controller: minio
        ports:
          http:
            port: 9001
          s3:
            port: 9000
    serviceMonitor:
      app:
        serviceName: minio
        endpoints:
          - port: s3
            scheme: http
            path: /minio/v2/metrics/cluster
            interval: 1m
            scrapeTimeout: 10s
    ingress:
      app:
        className: internal
        hosts:
          - host: "{{ .Release.Name }}.${SECRET_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: app
                  port: http
          - host: s3.${SECRET_DOMAIN}
            paths:
              - path: /
                service:
                  identifier: app
                  port: s3
    persistence:
      config:
        type: nfs
        server: shango.internal
        path: /mnt/user/k8s/minio
        globalMounts:
          - path: /data
      # data:
      #   # Use the existing NFS volume claim for storage
      #   # to use nfs version 3 while keeping the global config in 4
      #   existingClaim: nfs-minio-pvc
      #   globalMounts:
      #     - path: /data
